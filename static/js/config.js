window.config = {
  MODELS: [
    "Command A",
    "Command R",
    "Command R Plus",
    "DeepSeek-R1 1.5B",
    "DeepSeek-R1 3B",
    "DeepSeek-R1 7B",
    "DeepSeek-R1 8B",
    "DeepSeek-R1 14B",
    "DeepSeek-R1 32B",
    "DeepSeek-R1 70B",
    "DeepSeek-R1 671B",
    "DeepSeek-V3 671B",
    "ERNIE-4.5-0.3B",
    "ERNIE-4.5-0.3B-Base",
    "ERNIE-4.5-21B-A3B",
    "ERNIE-4.5-21B-A3B-Base",
    "ERNIE-4.5-300B-A47B",
    "ERNIE-4.5-300B-A47B-Base",
    "ERNIE-4.5-VL-28B-A3B",
    "ERNIE-4.5-VL-28B-A3B-Base",
    "ERNIE-4.5-VL-424B-A47B",
    "ERNIE-4.5-VL-424B-A47B-Base",
    "Falcon-1B",
    "Falcon-3B",
    "Falcon-7B",
    "Falcon-40B",
    "Falcon-180B",
    "Falcon2-11B",
    "Falcon3-1B",
    "Falcon3-3B",
    "Falcon3-7B",
    "Falcon3-10B",
    "Gemma 1 2B",
    "Gemma 1 7B",
    "Gemma 2 2B",
    "Gemma 2 9B",
    "Gemma 2 27B",
    "Gemma 3 1B",
    "Gemma 3 4B",
    "Gemma 3 12B",
    "Gemma 3 27B",
    "Gemma 3n E2B IT",
    "Kimi-Dev-72B",
    "Kimi K2-Base",
    "Kimi K2-Instruct",
    "Kimi-VL-A3B-Instruct",
    "Kimi-VL-A3B-Thinking",
    "Llama 3 8B",
    "Llama 3 70B",
    "Llama 3.1 8B",
    "Llama 3.1 70B",
    "Llama 3.1 405B",
    "Llama 3.2 1B",
    "Llama 3.2 3B",
    "Llama 3.3 70B",
    "Llama 4 Scout",
    "Llama 4 Maverick",
    "Llama 4 Behemoth",
    "Ministral-8B-2410",
    "Mistral-7B-Instruct-v0.1",
    "Mistral-7B-Instruct-v0.2",
    "Mistral-7B-v0.1",
    "Mistral-Large-2407",
    "Mistral-Small-2501",
    "Mixtral-8x7B-v0.1",
    "Mixtral-8x22B-v0.1",
    "Phi-1",
    "Phi-1.5",
    "Phi-2",
    "Phi-3-mini",
    "Phi-3-small",
    "Phi-3-medium",
    "Phi-4",
    "Phi-4-Mini",
    "Phi-4 Reasoning Plus",
    "Qwen2-0.5B",
    "Qwen2-1.5B",
    "Qwen2-7B",
    "Qwen2-72B",
    "Qwen2.5-0.5B",
    "Qwen2.5-1.5B",
    "Qwen2.5-3B",
    "Qwen2.5-7B",
    "Qwen2.5-14B",
    "Qwen2.5-32B",
    "Qwen2.5-72B",
    "Qwen3-0.6B",
    "Qwen3-1.7B",
    "Qwen3-4B",
    "Qwen3-8B",
    "Qwen3-14B",
    "Qwen3-30B-A3B",
    "Qwen3-32B",
    "Qwen3-235B-A22B",
    "Qwen3 235B A22B Thinking",
    "Qwen3 Coder 480B A35B",
  ],
  QUANTIZATION: [
    "FP16",
    "FP32",
    "INT8",
    "4-bit (QLoRA)"
  ],
  KV_CACHE_QUANTIZATION: [
    "FP16 / BF16",
    "FP8",
    "INT8",
    "INT4 (Experimental)"
  ],
  GPUS: {
    "NVIDIA RTX 3060 (12GB)": 12,
    "NVIDIA RTX 3060 Ti (8GB)": 8,
    "NVIDIA RTX 3070 (8GB)": 8,
    "NVIDIA RTX 3070 Ti (8GB)": 8,
    "NVIDIA RTX 3080 (10GB)": 10,
    "NVIDIA RTX 3080 (12GB)": 12,
    "NVIDIA RTX 3080 Ti (12GB)": 12,
    "NVIDIA RTX 3090 (24GB)": 24,
    "NVIDIA RTX 3090 Ti (24GB)": 24,
    "NVIDIA RTX 4060 (8GB)": 8,
    "NVIDIA RTX 4060 Ti (8GB)": 8,
    "NVIDIA RTX 4060 Ti (16GB)": 16,
    "NVIDIA RTX 4070 (12GB)": 12,
    "NVIDIA RTX 4070 Ti (12GB)": 12,
    "NVIDIA RTX 4070 Ti SUPER (16GB)": 16,
    "NVIDIA RTX 4070 SUPER (12GB)": 12,
    "NVIDIA RTX 4080 (16GB)": 16,
    "NVIDIA RTX 4080 SUPER (16GB)": 16,
    "NVIDIA RTX 4090 (24GB)": 24,
    "NVIDIA RTX 5070 (12GB)": 12,
    "NVIDIA RTX 5070 Ti (16GB)": 16,
    "NVIDIA RTX 5080 (16GB)": 16,
    "NVIDIA RTX 5090 (32GB)": 32,
    "NVIDIA RTX A4000 (16GB)": 16,
    "NVIDIA RTX A5000 (24GB)": 24,
    "NVIDIA RTX A6000 (48GB)": 48,
    "NVIDIA L40 (48GB)": 48,
    "NVIDIA L40S (48GB)": 48,
    "NVIDIA A2 (16GB)": 16,
    "NVIDIA A16 (64GB)": 64,
    "NVIDIA A30 (24GB)": 24,
    "NVIDIA A40 (48GB)": 48,
    "NVIDIA A100 (40GB)": 40,
    "NVIDIA A100 (80GB)": 80,
    "NVIDIA A800 (40GB)": 40,
    "NVIDIA A800 (80GB)": 80,
    "NVIDIA H100 (80GB)": 80,
    "NVIDIA H100 NVL (188GB)": 188,
    "NVIDIA H200 (141GB)": 141,
    "NVIDIA H800 (80GB)": 80,
    "NVIDIA B100 (192GB)": 192,
    "NVIDIA B200 (192GB)": 192,
    "Apple M2 Pro (16GB)": 16,
    "Apple M2 Max (32GB)": 32,
    "Apple M2 Max (64GB)": 64,
    "Apple M2 Max (96GB)": 96,
    "Apple M2 Ultra (64GB)": 64,
    "Apple M2 Ultra (128GB)": 128,
    "Apple M2 Ultra (192GB)": 192,
    "Apple M3 Pro (18GB)": 18,
    "Apple M3 Pro (36GB)": 36,
    "Apple M3 Max (36GB)": 36,
    "Apple M3 Max (48GB)": 48,
    "Apple M3 Max (64GB)": 64,
    "Apple M3 Max (96GB)": 96,
    "Apple M3 Max (128GB)": 128,
    "Apple M3 Ultra (256GB)": 256,
    "Apple M3 Ultra (512GB)": 512,
    "Apple M4 (16GB)": 16,
    "Apple M4 (24GB)": 24,
    "Apple M4 (32GB)": 32,
    "Apple M4 Pro (32GB)": 32,
    "Apple M4 Pro (64GB)": 64,
    "Apple M4 Max (64GB)": 64,
    "Apple M4 Max (96GB)": 96,
    "Apple M4 Max (128GB)": 128,
    "AMD MI300X": 192,
    "Google TPU v5p": 95,
  },
  MODEL_SIZES: {
    "Command A": 233.1,
    "Command R": 73.5,
    "Command R Plus": 218.4,

    "DeepSeek-R1 1.5B": 3.1,
    "DeepSeek-R1 3B": 6.3,
    "DeepSeek-R1 7B": 14.8,
    "DeepSeek-R1 8B": 16.9,
    "DeepSeek-R1 14B": 29.6,
    "DeepSeek-R1 32B": 67.2,
    "DeepSeek-R1 70B": 148.06,
    "DeepSeek-R1 671B": 1410,
    "DeepSeek-V3 671B": 1410,

    "ERNIE-4.5-0.3B": 0.63,
    "ERNIE-4.5-0.3B-Base": 0.63,
    "ERNIE-4.5-21B-A3B": 44.1,
    "ERNIE-4.5-21B-A3B-Base": 44.1,
    "ERNIE-4.5-300B-A47B": 630,
    "ERNIE-4.5-300B-A47B-Base": 630,
    "ERNIE-4.5-VL-28B-A3B": 58.8,
    "ERNIE-4.5-VL-28B-A3B-Base": 58.8,
    "ERNIE-4.5-VL-424B-A47B": 890.4,
    "ERNIE-4.5-VL-424B-A47B-Base": 890.4,

    "Falcon-1B": 2.1,
    "Falcon-3B": 6.3,
    "Falcon-7B": 14.8,
    "Falcon-40B": 84,
    "Falcon-180B": 378,
    "Falcon2-11B": 23.31,
    "Falcon3-1B": 2.1,
    "Falcon3-3B": 6.3,
    "Falcon3-7B": 14.8,
    "Falcon3-10B": 21,

    "Gemma 1 2B": 4.2,
    "Gemma 1 7B": 14.8,
    "Gemma 2 2B": 4.2,
    "Gemma 2 9B": 19,
    "Gemma 2 27B": 57,
    "Gemma 3 1B": 2.1,
    "Gemma 3 4B": 8.4,
    "Gemma 3 12B": 25.2,
    "Gemma 3 27B": 56.7,
    "Gemma 3n E2B IT": 4.2,

    "Kimi-Dev-72B": 151.2,
    "Kimi K2-Base": 2100,
    "Kimi K2-Instruct": 2100,
    "Kimi-VL-A3B-Instruct": 6.3,
    "Kimi-VL-A3B-Thinking": 6.3,

    "Llama 3 8B": 16.9,
    "Llama 3 70B": 148.06,
    "Llama 3.1 8B": 16.9,
    "Llama 3.1 70B": 148.06,
    "Llama 3.1 405B": 850.5,
    "Llama 3.2 1B": 2.1,
    "Llama 3.2 3B": 6.3,
    "Llama 3.3 70B": 148.06,
    "Llama 4 Scout": 230,
    "Llama 4 Maverick": 1184.36,
    "Llama 4 Behemoth": 4200,

    "Ministral-8B-2410": 25.2,
    "Mistral-7B-Instruct-v0.1": 14.8,
    "Mistral-7B-Instruct-v0.2": 14.8,
    "Mistral-7B-v0.1": 14.8,
    "Mistral-Large-2407": 258.3,
    "Mistral-Small-2501": 46.2,
    "Mixtral-8x7B-v0.1": 94.6,
    "Mixtral-8x22B-v0.1": 296.1,

    "Phi-1": 2.7,
    "Phi-1.5": 2.7,
    "Phi-2": 5.7,
    "Phi-3-mini": 8.0,
    "Phi-3-small": 14.8,
    "Phi-3-medium": 29.6,
    "Phi-4": 84,
    "Phi-4-Mini": 10.5,
    "Phi-4 Reasoning Plus": 105,

    "Qwen2-0.5B": 1.1,
    "Qwen2-1.5B": 3.1,
    "Qwen2-7B": 14.8,
    "Qwen2-72B": 152.6,
    "Qwen2.5-0.5B": 1.1,
    "Qwen2.5-1.5B": 3.1,
    "Qwen2.5-3B": 6.3,
    "Qwen2.5-7B": 14.8,
    "Qwen2.5-14B": 29.6,
    "Qwen2.5-32B": 67.8,
    "Qwen2.5-72B": 152.6,
    "Qwen3-0.6B": 1.26,
    "Qwen3-1.7B": 3.57,
    "Qwen3-4B": 8.4,
    "Qwen3-8B": 16.8,
    "Qwen3-14B": 29.4,
    "Qwen3-30B-A3B": 63,
    "Qwen3-32B": 67.2,
    "Qwen3-235B-A22B": 493.5,
    "Qwen3 235B A22B Thinking": 493.5,
    "Qwen3 Coder 480B A35B": 1008,
  },
  MODEL_ATTENTION: {
    "Command A": {
      attention: "MHA",
      embeddings: "rotary",
    },
    "Command R": {
      attention: "MHA",
      embeddings: "rotary",
    },
    "Command R Plus": {
      attention: "MHA",
      embeddings: "rotary",
    },
    "DeepSeek-R1 1.5B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "DeepSeek-R1 3B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "DeepSeek-R1 7B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "DeepSeek-R1 8B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "DeepSeek-R1 14B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "DeepSeek-R1 32B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "DeepSeek-R1 70B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "DeepSeek-R1 671B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "DeepSeek-V3 671B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "ERNIE-4.5-0.3B": {
      attention: "MHA",
      embeddings: "learned",
    },
    "ERNIE-4.5-0.3B-Base": {
      attention: "MHA",
      embeddings: "learned",
    },
    "ERNIE-4.5-21B-A3B": {
      attention: "MoE",
      embeddings: "learned",
    },
    "ERNIE-4.5-21B-A3B-Base": {
      attention: "MoE",
      embeddings: "learned",
    },
    "ERNIE-4.5-300B-A47B": {
      attention: "MoE",
      embeddings: "learned",
    },
    "ERNIE-4.5-300B-A47B-Base": {
      attention: "MoE",
      embeddings: "learned",
    },
    "ERNIE-4.5-VL-28B-A3B": {
      attention: "MoE",
      embeddings: "learned",
    },
    "ERNIE-4.5-VL-28B-A3B-Base": {
      attention: "MoE",
      embeddings: "learned",
    },
    "ERNIE-4.5-VL-424B-A47B": {
      attention: "MoE",
      embeddings: "learned",
    },
    "ERNIE-4.5-VL-424B-A47B-Base": {
      attention: "MoE",
      embeddings: "learned",
    },
    "Falcon-1B": {
      attention: "MHA",
      embeddings: "alibi",
    },
    "Falcon-3B": {
      attention: "MHA",
      embeddings: "alibi",
    },
    "Falcon-7B": {
      attention: "MHA",
      embeddings: "alibi",
    },
    "Falcon-40B": {
      attention: "MHA",
      embeddings: "alibi",
    },
    "Falcon-180B": {
      attention: "MHA",
      embeddings: "alibi",
    },
    "Falcon2-11B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Falcon3-1B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Falcon3-3B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Falcon3-7B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Falcon3-10B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Gemma 1 2B": {
      attention: "MQA",
      embeddings: "rope",
    },
    "Gemma 1 7B": {
      attention: "MQA",
      embeddings: "rope",
    },
    "Gemma 2 2B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Gemma 2 9B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Gemma 2 27B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Gemma 3 1B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Gemma 3 4B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Gemma 3 12B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Gemma 3 27B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Gemma 3n E2B IT": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Kimi-Dev-72B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Kimi K2-Base": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Kimi K2-Instruct": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Kimi-VL-A3B-Instruct": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Kimi-VL-A3B-Thinking": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Llama 3 8B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Llama 3 70B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Llama 3.1 8B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Llama 3.1 70B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Llama 3.1 405B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Llama 3.2 1B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Llama 3.2 3B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Llama 3.3 70B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Llama 4 Scout": {
      attention: "GQA",
      embeddings: "irope",
    },
    "Llama 4 Maverick": {
      attention: "GQA",
      embeddings: "irope",
    },
    "Llama 4 Behemoth": {
      attention: "GQA",
      embeddings: "absolute",
    },
    "Ministral-8B-2410": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Mistral-7B-Instruct-v0.1": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Mistral-7B-Instruct-v0.2": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Mistral-7B-v0.1": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Mistral-Large-2407": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Mistral-Small-2501": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Mixtral-8x7B-v0.1": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Mixtral-8x22B-v0.1": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Phi-1": {
      attention: "MHA",
      embeddings: "learned",
    },
    "Phi-1.5": {
      attention: "MHA",
      embeddings: "learned",
    },
    "Phi-2": {
      attention: "MHA",
      embeddings: "learned",
    },
    "Phi-3-mini": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Phi-3-small": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Phi-3-medium": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Phi-4": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Phi-4-Mini": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Phi-4 Reasoning Plus": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2-0.5B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2-1.5B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2-7B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2-72B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2.5-0.5B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2.5-1.5B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2.5-3B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2.5-7B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2.5-14B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2.5-32B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2.5-72B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen3-0.6B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen3-1.7B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen3-4B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen3-8B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen3-14B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen3-30B-A3B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Qwen3-32B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen3-235B-A22B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Qwen3 235B A22B Thinking": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Qwen3 Coder 480B A35B": {
      attention: "MoE",
      embeddings: "rope",
    },
  },
}
