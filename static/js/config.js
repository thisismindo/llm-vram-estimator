window.config = {
  MODELS: [
    "Command A",
    "Command R",
    "Command R Plus",
    "Command R7B",
    "DeepSeek-R1 1.5B",
    "DeepSeek-R1 3B",
    "DeepSeek-R1 7B",
    "DeepSeek-R1 8B",
    "DeepSeek-R1 14B",
    "DeepSeek-R1 32B",
    "DeepSeek-R1 70B",
    "DeepSeek-R1 671B",
    "DeepSeek-R1-0528 671B",
    "DeepSeek-V3 671B",
    "DeepSeek-V3.2 685B",
    "DeepSeek-V3.2-Speciale 685B",
    "Devstral-Small-24B",
    "Devstral-2-123B",
    "Devstral-Small-2-24B",
    "ERNIE-4.5-0.3B",
    "ERNIE-4.5-0.3B-Base",
    "ERNIE-4.5-21B-A3B",
    "ERNIE-4.5-21B-A3B-Base",
    "ERNIE-4.5-300B-A47B",
    "ERNIE-4.5-300B-A47B-Base",
    "ERNIE-4.5-VL-28B-A3B",
    "ERNIE-4.5-VL-28B-A3B-Base",
    "ERNIE-4.5-VL-424B-A47B",
    "ERNIE-4.5-VL-424B-A47B-Base",
    "Falcon-1B",
    "Falcon-3B",
    "Falcon-7B",
    "Falcon-40B",
    "Falcon-180B",
    "Falcon2-11B",
    "Falcon3-1B",
    "Falcon3-3B",
    "Falcon3-7B",
    "Falcon3-10B",
    "Gemma 1 2B",
    "Gemma 1 7B",
    "Gemma 2 2B",
    "Gemma 2 9B",
    "Gemma 2 27B",
    "Gemma 3 1B",
    "Gemma 3 4B",
    "Gemma 3 12B",
    "Gemma 3 27B",
    "Gemma 3n E2B IT",
    "GLM-4.5-355B-A32B",
    "GLM-4.5-Air-106B-A12B",
    "GLM-4.6-355B",
    "GLM-4.7-355B",
    "GLM-5-744B",
    "GPT-oss-20B",
    "GPT-oss-120B",
    "Hunyuan-A13B",
    "InternLM3-8B",
    "Kimi-Dev-72B",
    "Kimi K2-Base",
    "Kimi K2-Instruct",
    "Kimi K2.5",
    "Kimi-VL-A3B-Instruct",
    "Kimi-VL-A3B-Thinking",
    "Llama 3 8B",
    "Llama 3 70B",
    "Llama 3.1 8B",
    "Llama 3.1 70B",
    "Llama 3.1 405B",
    "Llama 3.2 1B",
    "Llama 3.2 3B",
    "Llama 3.3 70B",
    "Llama 4 Scout",
    "Llama 4 Maverick",
    "Llama 4 Behemoth",
    "MiMo-V2-Flash-309B",
    "MiniMax-M2-230B",
    "MiniMax-M2.1-230B",
    "Ministral-3-3B",
    "Ministral-3-8B",
    "Ministral-3-14B",
    "Ministral-8B-2410",
    "Mistral-7B-Instruct-v0.1",
    "Mistral-7B-Instruct-v0.2",
    "Mistral-7B-v0.1",
    "Mistral-Large-2407",
    "Mistral-Large-3-675B",
    "Mistral-Small-2501",
    "Mistral-Small-3.1-24B",
    "Mistral-Small-3.2-24B",
    "Mixtral-8x7B-v0.1",
    "Mixtral-8x22B-v0.1",
    "Nemotron-3-Nano-30B-A3B",
    "OLMo-2-1B",
    "OLMo-2-7B",
    "OLMo-2-13B",
    "OLMo-2-32B",
    "OLMo-3-7B",
    "OLMo-3-32B",
    "Phi-1",
    "Phi-1.5",
    "Phi-2",
    "Phi-3-mini",
    "Phi-3-small",
    "Phi-3-medium",
    "Phi-4",
    "Phi-4-Mini",
    "Phi-4 Reasoning Plus",
    "Qwen2-0.5B",
    "Qwen2-1.5B",
    "Qwen2-7B",
    "Qwen2-72B",
    "Qwen2.5-0.5B",
    "Qwen2.5-1.5B",
    "Qwen2.5-3B",
    "Qwen2.5-7B",
    "Qwen2.5-14B",
    "Qwen2.5-32B",
    "Qwen2.5-72B",
    "Qwen3-0.6B",
    "Qwen3-1.7B",
    "Qwen3-4B",
    "Qwen3-8B",
    "Qwen3-14B",
    "Qwen3-30B-A3B",
    "Qwen3-32B",
    "Qwen3-235B-A22B",
    "Qwen3 235B A22B Thinking",
    "Qwen3 Coder 480B A35B",
    "Qwen3.5-397B-A17B",
    "QwQ-32B",
    "SmolLM2-135M",
    "SmolLM2-360M",
    "SmolLM2-1.7B",
    "SmolLM3-3B",
  ],
  QUANTIZATION: [
    "FP32",
    "BF16",
    "FP16",
    "FP8",
    "INT8",
    "4-bit (QLoRA)"
  ],
  KV_CACHE_QUANTIZATION: [
    "FP16 / BF16",
    "FP8",
    "INT8",
    "INT4 (Experimental)"
  ],
  GPUS: {
    "NVIDIA RTX 3060 (12GB)": 12,
    "NVIDIA RTX 3060 Ti (8GB)": 8,
    "NVIDIA RTX 3070 (8GB)": 8,
    "NVIDIA RTX 3070 Ti (8GB)": 8,
    "NVIDIA RTX 3080 (10GB)": 10,
    "NVIDIA RTX 3080 (12GB)": 12,
    "NVIDIA RTX 3080 Ti (12GB)": 12,
    "NVIDIA RTX 3090 (24GB)": 24,
    "NVIDIA RTX 3090 Ti (24GB)": 24,
    "NVIDIA RTX 4060 (8GB)": 8,
    "NVIDIA RTX 4060 Ti (8GB)": 8,
    "NVIDIA RTX 4060 Ti (16GB)": 16,
    "NVIDIA RTX 4070 (12GB)": 12,
    "NVIDIA RTX 4070 Ti (12GB)": 12,
    "NVIDIA RTX 4070 Ti SUPER (16GB)": 16,
    "NVIDIA RTX 4070 SUPER (12GB)": 12,
    "NVIDIA RTX 4080 (16GB)": 16,
    "NVIDIA RTX 4080 SUPER (16GB)": 16,
    "NVIDIA RTX 4090 (24GB)": 24,
    "NVIDIA RTX 5060 (8GB)": 8,
    "NVIDIA RTX 5060 Ti (8GB)": 8,
    "NVIDIA RTX 5060 Ti (16GB)": 16,
    "NVIDIA RTX 5070 (12GB)": 12,
    "NVIDIA RTX 5070 Ti (16GB)": 16,
    "NVIDIA RTX 5080 (16GB)": 16,
    "NVIDIA RTX 5090 (32GB)": 32,
    "NVIDIA RTX PRO 5000 Blackwell (48GB)": 48,
    "NVIDIA RTX PRO 5000 Blackwell (72GB)": 72,
    "NVIDIA RTX PRO 6000 Blackwell (96GB)": 96,
    "NVIDIA RTX A4000 (16GB)": 16,
    "NVIDIA RTX A5000 (24GB)": 24,
    "NVIDIA RTX A6000 (48GB)": 48,
    "NVIDIA L40 (48GB)": 48,
    "NVIDIA L40S (48GB)": 48,
    "NVIDIA A2 (16GB)": 16,
    "NVIDIA A16 (64GB)": 64,
    "NVIDIA A30 (24GB)": 24,
    "NVIDIA A40 (48GB)": 48,
    "NVIDIA A100 (40GB)": 40,
    "NVIDIA A100 (80GB)": 80,
    "NVIDIA A800 (40GB)": 40,
    "NVIDIA A800 (80GB)": 80,
    "NVIDIA H100 (80GB)": 80,
    "NVIDIA H100 NVL (188GB)": 188,
    "NVIDIA H200 (141GB)": 141,
    "NVIDIA H800 (80GB)": 80,
    "NVIDIA B100 (192GB)": 192,
    "NVIDIA B200 (192GB)": 192,
    "NVIDIA B300 (288GB)": 288,
    "Apple M2 Ultra (64GB)": 64,
    "Apple M2 Ultra (128GB)": 128,
    "Apple M2 Ultra (192GB)": 192,
    "Apple M3 Ultra (96GB)": 96,
    "Apple M3 Ultra (256GB)": 256,
    "Apple M3 Ultra (512GB)": 512,
    "Apple M4 Max (36GB)": 36,
    "Apple M4 Max (48GB)": 48,
    "Apple M4 Max (64GB)": 64,
    "Apple M4 Max (128GB)": 128,
    "Apple M4 Pro (16GB)": 16,
    "Apple M4 Pro (24GB)": 24,
    "Apple M4 Pro (32GB)": 32,
    "Apple M4 Pro (36GB)": 36,
    "Apple M4 Pro (48GB)": 48,
    "Apple M5 (16GB)": 16,
    "Apple M5 (24GB)": 24,
    "Apple M5 (32GB)": 32,
    "AMD Instinct MI300X (192GB)": 192,
    "AMD Instinct MI325X (256GB)": 256,
    "AMD Instinct MI350X (288GB)": 288,
    "AMD Instinct MI355X (288GB)": 288,
    "AMD Radeon RX 9070 (16GB)": 16,
    "AMD Radeon RX 9070 XT (16GB)": 16,
    "Google TPU v5p (95GB)": 95,
    "Google TPU v6e Trillium (32GB)": 32,
    "Google TPU v7 Ironwood (192GB)": 192,
    "Intel Gaudi 3 (128GB)": 128,
    "Intel Arc B570 (10GB)": 10,
    "Intel Arc B580 (12GB)": 12,
    "AWS Trainium2 (96GB)": 96,
    "AWS Trainium3 (144GB)": 144,
  },
  GPU_TDP: {
    "NVIDIA RTX 3060 (12GB)": 170,
    "NVIDIA RTX 3060 Ti (8GB)": 200,
    "NVIDIA RTX 3070 (8GB)": 220,
    "NVIDIA RTX 3070 Ti (8GB)": 290,
    "NVIDIA RTX 3080 (10GB)": 320,
    "NVIDIA RTX 3080 (12GB)": 350,
    "NVIDIA RTX 3080 Ti (12GB)": 350,
    "NVIDIA RTX 3090 (24GB)": 350,
    "NVIDIA RTX 3090 Ti (24GB)": 450,
    "NVIDIA RTX 4060 (8GB)": 115,
    "NVIDIA RTX 4060 Ti (8GB)": 160,
    "NVIDIA RTX 4060 Ti (16GB)": 165,
    "NVIDIA RTX 4070 (12GB)": 200,
    "NVIDIA RTX 4070 Ti (12GB)": 285,
    "NVIDIA RTX 4070 Ti SUPER (16GB)": 285,
    "NVIDIA RTX 4070 SUPER (12GB)": 220,
    "NVIDIA RTX 4080 (16GB)": 320,
    "NVIDIA RTX 4080 SUPER (16GB)": 320,
    "NVIDIA RTX 4090 (24GB)": 450,
    "NVIDIA RTX 5060 (8GB)": 150,
    "NVIDIA RTX 5060 Ti (8GB)": 180,
    "NVIDIA RTX 5060 Ti (16GB)": 180,
    "NVIDIA RTX 5070 (12GB)": 250,
    "NVIDIA RTX 5070 Ti (16GB)": 300,
    "NVIDIA RTX 5080 (16GB)": 360,
    "NVIDIA RTX 5090 (32GB)": 575,
    "NVIDIA RTX PRO 5000 Blackwell (48GB)": 250,
    "NVIDIA RTX PRO 5000 Blackwell (72GB)": 300,
    "NVIDIA RTX PRO 6000 Blackwell (96GB)": 350,
    "NVIDIA RTX A4000 (16GB)": 140,
    "NVIDIA RTX A5000 (24GB)": 230,
    "NVIDIA RTX A6000 (48GB)": 300,
    "NVIDIA L40 (48GB)": 300,
    "NVIDIA L40S (48GB)": 350,
    "NVIDIA A2 (16GB)": 60,
    "NVIDIA A16 (64GB)": 250,
    "NVIDIA A30 (24GB)": 165,
    "NVIDIA A40 (48GB)": 300,
    "NVIDIA A100 (40GB)": 400,
    "NVIDIA A100 (80GB)": 400,
    "NVIDIA A800 (40GB)": 300,
    "NVIDIA A800 (80GB)": 300,
    "NVIDIA H100 (80GB)": 700,
    "NVIDIA H100 NVL (188GB)": 800,
    "NVIDIA H200 (141GB)": 700,
    "NVIDIA H800 (80GB)": 700,
    "NVIDIA B100 (192GB)": 700,
    "NVIDIA B200 (192GB)": 1000,
    "NVIDIA B300 (288GB)": 1200,
    "Apple M2 Ultra (64GB)": 295,
    "Apple M2 Ultra (128GB)": 295,
    "Apple M2 Ultra (192GB)": 295,
    "Apple M3 Ultra (96GB)": 270,
    "Apple M3 Ultra (256GB)": 270,
    "Apple M3 Ultra (512GB)": 270,
    "Apple M4 Max (36GB)": 145,
    "Apple M4 Max (48GB)": 145,
    "Apple M4 Max (64GB)": 145,
    "Apple M4 Max (128GB)": 145,
    "Apple M4 Pro (16GB)": 70,
    "Apple M4 Pro (24GB)": 70,
    "Apple M4 Pro (32GB)": 70,
    "Apple M4 Pro (36GB)": 70,
    "Apple M4 Pro (48GB)": 70,
    "Apple M5 (16GB)": 70,
    "Apple M5 (24GB)": 70,
    "Apple M5 (32GB)": 70,
    "AMD Instinct MI300X (192GB)": 750,
    "AMD Instinct MI325X (256GB)": 1000,
    "AMD Instinct MI350X (288GB)": 1000,
    "AMD Instinct MI355X (288GB)": 1400,
    "AMD Radeon RX 9070 (16GB)": 200,
    "AMD Radeon RX 9070 XT (16GB)": 250,
    "Google TPU v5p (95GB)": 400,
    "Google TPU v6e Trillium (32GB)": 350,
    "Google TPU v7 Ironwood (192GB)": 500,
    "Intel Gaudi 3 (128GB)": 600,
    "Intel Arc B570 (10GB)": 150,
    "Intel Arc B580 (12GB)": 190,
    "AWS Trainium2 (96GB)": 400,
    "AWS Trainium3 (144GB)": 500,
  },
  MODEL_SIZES: {
    "Command A": 233.1,
    "Command R": 73.5,
    "Command R Plus": 218.4,
    "Command R7B": 14.7,

    "DeepSeek-R1 1.5B": 3.1,
    "DeepSeek-R1 3B": 6.3,
    "DeepSeek-R1 7B": 14.8,
    "DeepSeek-R1 8B": 16.9,
    "DeepSeek-R1 14B": 29.6,
    "DeepSeek-R1 32B": 67.2,
    "DeepSeek-R1 70B": 148.06,
    "DeepSeek-R1 671B": 1410,
    "DeepSeek-R1-0528 671B": 1410,
    "DeepSeek-V3 671B": 1410,
    "DeepSeek-V3.2 685B": 1438.5,
    "DeepSeek-V3.2-Speciale 685B": 1438.5,

    "Devstral-Small-24B": 50.4,
    "Devstral-2-123B": 258.3,
    "Devstral-Small-2-24B": 50.4,

    "ERNIE-4.5-0.3B": 0.63,
    "ERNIE-4.5-0.3B-Base": 0.63,
    "ERNIE-4.5-21B-A3B": 44.1,
    "ERNIE-4.5-21B-A3B-Base": 44.1,
    "ERNIE-4.5-300B-A47B": 630,
    "ERNIE-4.5-300B-A47B-Base": 630,
    "ERNIE-4.5-VL-28B-A3B": 58.8,
    "ERNIE-4.5-VL-28B-A3B-Base": 58.8,
    "ERNIE-4.5-VL-424B-A47B": 890.4,
    "ERNIE-4.5-VL-424B-A47B-Base": 890.4,

    "Falcon-1B": 2.1,
    "Falcon-3B": 6.3,
    "Falcon-7B": 14.8,
    "Falcon-40B": 84,
    "Falcon-180B": 378,
    "Falcon2-11B": 23.31,
    "Falcon3-1B": 2.1,
    "Falcon3-3B": 6.3,
    "Falcon3-7B": 14.8,
    "Falcon3-10B": 21,

    "Gemma 1 2B": 4.2,
    "Gemma 1 7B": 14.8,
    "Gemma 2 2B": 4.2,
    "Gemma 2 9B": 19,
    "Gemma 2 27B": 57,
    "Gemma 3 1B": 2.1,
    "Gemma 3 4B": 8.4,
    "Gemma 3 12B": 25.2,
    "Gemma 3 27B": 56.7,
    "Gemma 3n E2B IT": 4.2,

    "GLM-4.5-355B-A32B": 745.5,
    "GLM-4.5-Air-106B-A12B": 222.6,
    "GLM-4.6-355B": 745.5,
    "GLM-4.7-355B": 745.5,
    "GLM-5-744B": 1562.4,

    "GPT-oss-20B": 44.1,
    "GPT-oss-120B": 245.7,

    "Hunyuan-A13B": 168,

    "InternLM3-8B": 16.8,

    "Kimi-Dev-72B": 151.2,
    "Kimi K2-Base": 2100,
    "Kimi K2-Instruct": 2100,
    "Kimi K2.5": 2184,
    "Kimi-VL-A3B-Instruct": 6.3,
    "Kimi-VL-A3B-Thinking": 6.3,

    "Llama 3 8B": 16.9,
    "Llama 3 70B": 148.06,
    "Llama 3.1 8B": 16.9,
    "Llama 3.1 70B": 148.06,
    "Llama 3.1 405B": 850.5,
    "Llama 3.2 1B": 2.1,
    "Llama 3.2 3B": 6.3,
    "Llama 3.3 70B": 148.06,
    "Llama 4 Scout": 230,
    "Llama 4 Maverick": 1184.36,
    "Llama 4 Behemoth": 4200,

    "MiMo-V2-Flash-309B": 648.9,

    "MiniMax-M2-230B": 483,
    "MiniMax-M2.1-230B": 483,

    "Ministral-3-3B": 6.3,
    "Ministral-3-8B": 16.8,
    "Ministral-3-14B": 29.4,
    "Ministral-8B-2410": 25.2,
    "Mistral-7B-Instruct-v0.1": 14.8,
    "Mistral-7B-Instruct-v0.2": 14.8,
    "Mistral-7B-v0.1": 14.8,
    "Mistral-Large-2407": 258.3,
    "Mistral-Large-3-675B": 1417.5,
    "Mistral-Small-2501": 46.2,
    "Mistral-Small-3.1-24B": 50.4,
    "Mistral-Small-3.2-24B": 50.4,
    "Mixtral-8x7B-v0.1": 94.6,
    "Mixtral-8x22B-v0.1": 296.1,

    "Nemotron-3-Nano-30B-A3B": 66.4,

    "OLMo-2-1B": 2.1,
    "OLMo-2-7B": 14.7,
    "OLMo-2-13B": 27.3,
    "OLMo-2-32B": 67.2,
    "OLMo-3-7B": 14.7,
    "OLMo-3-32B": 67.2,

    "Phi-1": 2.7,
    "Phi-1.5": 2.7,
    "Phi-2": 5.7,
    "Phi-3-mini": 8.0,
    "Phi-3-small": 14.8,
    "Phi-3-medium": 29.6,
    "Phi-4": 84,
    "Phi-4-Mini": 10.5,
    "Phi-4 Reasoning Plus": 105,

    "Qwen2-0.5B": 1.1,
    "Qwen2-1.5B": 3.1,
    "Qwen2-7B": 14.8,
    "Qwen2-72B": 152.6,
    "Qwen2.5-0.5B": 1.1,
    "Qwen2.5-1.5B": 3.1,
    "Qwen2.5-3B": 6.3,
    "Qwen2.5-7B": 14.8,
    "Qwen2.5-14B": 29.6,
    "Qwen2.5-32B": 67.8,
    "Qwen2.5-72B": 152.6,
    "Qwen3-0.6B": 1.26,
    "Qwen3-1.7B": 3.57,
    "Qwen3-4B": 8.4,
    "Qwen3-8B": 16.8,
    "Qwen3-14B": 29.4,
    "Qwen3-30B-A3B": 63,
    "Qwen3-32B": 67.2,
    "Qwen3-235B-A22B": 493.5,
    "Qwen3 235B A22B Thinking": 493.5,
    "Qwen3 Coder 480B A35B": 1008,
    "Qwen3.5-397B-A17B": 833.7,

    "QwQ-32B": 68.3,

    "SmolLM2-135M": 0.28,
    "SmolLM2-360M": 0.76,
    "SmolLM2-1.7B": 3.57,
    "SmolLM3-3B": 6.3,
  },
  CPUS: {
    "AMD EPYC 9754 (128C/256T)": { cores: 128, memChannels: 12, tier: "server", socket: "SP5", tdp: 360 },
    "AMD EPYC 9654 (96C/192T)": { cores: 96, memChannels: 12, tier: "server", socket: "SP5", tdp: 360 },
    "AMD EPYC 9554 (64C/128T)": { cores: 64, memChannels: 12, tier: "server", socket: "SP5", tdp: 280 },
    "AMD EPYC 9354 (32C/64T)": { cores: 32, memChannels: 12, tier: "server", socket: "SP5", tdp: 240 },
    "Intel Xeon w9-3595X (60C/120T)": { cores: 60, memChannels: 8, tier: "server", socket: "LGA4677", tdp: 350 },
    "Intel Xeon 8580 (60C/120T)": { cores: 60, memChannels: 8, tier: "server", socket: "LGA4677", tdp: 350 },
    "Intel Xeon 8490H (60C/120T)": { cores: 60, memChannels: 8, tier: "server", socket: "LGA4677", tdp: 350 },
    "AMD Threadripper PRO 7995WX (96C/192T)": { cores: 96, memChannels: 8, tier: "workstation", socket: "sWRX90", tdp: 350 },
    "AMD Threadripper PRO 7975WX (32C/64T)": { cores: 32, memChannels: 8, tier: "workstation", socket: "sWRX90", tdp: 280 },
    "AMD Threadripper 7980X (64C/128T)": { cores: 64, memChannels: 4, tier: "workstation", socket: "sTR5", tdp: 350 },
    "AMD Threadripper 7960X (24C/48T)": { cores: 24, memChannels: 4, tier: "workstation", socket: "sTR5", tdp: 280 },
    "AMD Ryzen 9 9950X (16C/32T)": { cores: 16, memChannels: 2, tier: "desktop", socket: "AM5", tdp: 170 },
    "AMD Ryzen 9 9900X (12C/24T)": { cores: 12, memChannels: 2, tier: "desktop", socket: "AM5", tdp: 120 },
    "AMD Ryzen 9 7950X (16C/32T)": { cores: 16, memChannels: 2, tier: "desktop", socket: "AM5", tdp: 170 },
    "AMD Ryzen 7 9700X (8C/16T)": { cores: 8, memChannels: 2, tier: "desktop", socket: "AM5", tdp: 65 },
    "AMD Ryzen 7 7800X3D (8C/16T)": { cores: 8, memChannels: 2, tier: "desktop", socket: "AM5", tdp: 120 },
    "AMD Ryzen 5 7600X (6C/12T)": { cores: 6, memChannels: 2, tier: "desktop", socket: "AM5", tdp: 105 },
    "Intel Core Ultra 9 285K (24C/24T)": { cores: 24, memChannels: 2, tier: "desktop", socket: "LGA1851", tdp: 125 },
    "Intel Core i9-14900K (24C/32T)": { cores: 24, memChannels: 2, tier: "desktop", socket: "LGA1700", tdp: 125 },
    "Intel Core i7-14700K (20C/28T)": { cores: 20, memChannels: 2, tier: "desktop", socket: "LGA1700", tdp: 125 },
    "Intel Core i5-14600K (14C/20T)": { cores: 14, memChannels: 2, tier: "desktop", socket: "LGA1700", tdp: 125 },
    "Intel Core i9-13900K (24C/32T)": { cores: 24, memChannels: 2, tier: "desktop", socket: "LGA1700", tdp: 125 },
    "Apple M2 Pro": { cores: 12, memChannels: 0, tier: "apple", socket: "Apple", tdp: 0 },
    "Apple M2 Max": { cores: 12, memChannels: 0, tier: "apple", socket: "Apple", tdp: 0 },
    "Apple M2 Ultra": { cores: 24, memChannels: 0, tier: "apple", socket: "Apple", tdp: 0 },
    "Apple M3 Pro": { cores: 12, memChannels: 0, tier: "apple", socket: "Apple", tdp: 0 },
    "Apple M3 Max": { cores: 16, memChannels: 0, tier: "apple", socket: "Apple", tdp: 0 },
    "Apple M3 Ultra": { cores: 32, memChannels: 0, tier: "apple", socket: "Apple", tdp: 0 },
    "Apple M4": { cores: 10, memChannels: 0, tier: "apple", socket: "Apple", tdp: 0 },
    "Apple M4 Pro": { cores: 14, memChannels: 0, tier: "apple", socket: "Apple", tdp: 0 },
    "Apple M4 Max": { cores: 16, memChannels: 0, tier: "apple", socket: "Apple", tdp: 0 },
    "Apple M5": { cores: 12, memChannels: 0, tier: "apple", socket: "Apple", tdp: 0 },
  },
  MOTHERBOARDS: {
    "Supermicro H13DSH (Dual SP5)": { socket: "SP5", chipset: "SP5", maxCPUs: 2, maxGPUs: 8, maxDIMMs: 24, maxRAMPerDIMM: 2048, supportedRAMTypes: ["DDR5-4800"], tier: "server", basePower: 60 },
    "Supermicro H13SSL-N (Single SP5)": { socket: "SP5", chipset: "SP5", maxCPUs: 1, maxGPUs: 4, maxDIMMs: 12, maxRAMPerDIMM: 2048, supportedRAMTypes: ["DDR5-4800"], tier: "server", basePower: 50 },
    "Supermicro X13DEI (Dual LGA4677)": { socket: "LGA4677", chipset: "LGA4677", maxCPUs: 2, maxGPUs: 8, maxDIMMs: 16, maxRAMPerDIMM: 2048, supportedRAMTypes: ["DDR5-4800"], tier: "server", basePower: 60 },
    "Supermicro X13SWA (Single LGA4677)": { socket: "LGA4677", chipset: "LGA4677", maxCPUs: 1, maxGPUs: 4, maxDIMMs: 8, maxRAMPerDIMM: 2048, supportedRAMTypes: ["DDR5-4800"], tier: "server", basePower: 50 },
    "ASUS Pro WS WRX90E-SAGE SE": { socket: "sWRX90", chipset: "WRX90", maxCPUs: 1, maxGPUs: 7, maxDIMMs: 8, maxRAMPerDIMM: 1024, supportedRAMTypes: ["DDR5-5600", "DDR5-4800"], tier: "workstation", basePower: 45 },
    "Gigabyte WRX90 AORUS MASTER": { socket: "sWRX90", chipset: "WRX90", maxCPUs: 1, maxGPUs: 4, maxDIMMs: 8, maxRAMPerDIMM: 1024, supportedRAMTypes: ["DDR5-5600", "DDR5-4800"], tier: "workstation", basePower: 45 },
    "ASUS TRX50-WS": { socket: "sTR5", chipset: "TRX50", maxCPUs: 1, maxGPUs: 4, maxDIMMs: 4, maxRAMPerDIMM: 256, supportedRAMTypes: ["DDR5-5600", "DDR5-4800"], tier: "workstation", basePower: 40 },
    "MSI TRX50 PRO WIFI": { socket: "sTR5", chipset: "TRX50", maxCPUs: 1, maxGPUs: 3, maxDIMMs: 4, maxRAMPerDIMM: 256, supportedRAMTypes: ["DDR5-5600", "DDR5-4800"], tier: "workstation", basePower: 40 },
    "ASUS ROG Crosshair X870E Hero": { socket: "AM5", chipset: "X870E", maxCPUs: 1, maxGPUs: 2, maxDIMMs: 4, maxRAMPerDIMM: 64, supportedRAMTypes: ["DDR5-4800", "DDR5-5600", "DDR5-6000", "DDR5-6400", "DDR5-7200"], tier: "desktop", maxTotalRAM: 256, basePower: 30 },
    "MSI MEG X870E ACE": { socket: "AM5", chipset: "X870E", maxCPUs: 1, maxGPUs: 2, maxDIMMs: 4, maxRAMPerDIMM: 64, supportedRAMTypes: ["DDR5-4800", "DDR5-5600", "DDR5-6000", "DDR5-6400", "DDR5-7200"], tier: "desktop", maxTotalRAM: 256, basePower: 30 },
    "Gigabyte X870E AORUS Master": { socket: "AM5", chipset: "X870E", maxCPUs: 1, maxGPUs: 2, maxDIMMs: 4, maxRAMPerDIMM: 64, supportedRAMTypes: ["DDR5-4800", "DDR5-5600", "DDR5-6000", "DDR5-6400", "DDR5-7200"], tier: "desktop", maxTotalRAM: 256, basePower: 30 },
    "ASUS ROG STRIX B650E-F": { socket: "AM5", chipset: "B650E", maxCPUs: 1, maxGPUs: 2, maxDIMMs: 4, maxRAMPerDIMM: 64, supportedRAMTypes: ["DDR5-4800", "DDR5-5600", "DDR5-6000", "DDR5-6400"], tier: "desktop", maxTotalRAM: 128, basePower: 25 },
    "ASUS ROG Maximus Z890 Hero": { socket: "LGA1851", chipset: "Z890", maxCPUs: 1, maxGPUs: 2, maxDIMMs: 4, maxRAMPerDIMM: 64, supportedRAMTypes: ["DDR5-4800", "DDR5-5600", "DDR5-6000", "DDR5-6400", "DDR5-7200"], tier: "desktop", maxTotalRAM: 256, basePower: 30 },
    "MSI MEG Z890 ACE": { socket: "LGA1851", chipset: "Z890", maxCPUs: 1, maxGPUs: 2, maxDIMMs: 4, maxRAMPerDIMM: 64, supportedRAMTypes: ["DDR5-4800", "DDR5-5600", "DDR5-6000", "DDR5-6400", "DDR5-7200"], tier: "desktop", maxTotalRAM: 256, basePower: 30 },
    "ASUS ROG Maximus Z790 Hero": { socket: "LGA1700", chipset: "Z790", maxCPUs: 1, maxGPUs: 2, maxDIMMs: 4, maxRAMPerDIMM: 64, supportedRAMTypes: ["DDR5-4800", "DDR5-5600", "DDR5-6000", "DDR5-6400", "DDR5-7200"], tier: "desktop", maxTotalRAM: 192, basePower: 30 },
    "MSI MAG Z690 TOMAHAWK": { socket: "LGA1700", chipset: "Z690", maxCPUs: 1, maxGPUs: 2, maxDIMMs: 4, maxRAMPerDIMM: 64, supportedRAMTypes: ["DDR5-4800", "DDR5-5600", "DDR5-6000"], tier: "desktop", maxTotalRAM: 128, basePower: 25 },
    "Apple Mac Studio": {
      socket: "Apple", chipset: "Apple", maxCPUs: 1, maxGPUs: 8, maxDIMMs: 0, maxRAMPerDIMM: 0, supportedRAMTypes: [], tier: "apple", basePower: 0,
      supportedCPUs: ["Apple M4 Max", "Apple M3 Ultra"],
      supportedGPUs: ["Apple M4 Max (36GB)", "Apple M4 Max (48GB)", "Apple M4 Max (64GB)", "Apple M4 Max (128GB)", "Apple M3 Ultra (96GB)", "Apple M3 Ultra (256GB)", "Apple M3 Ultra (512GB)"]
    },
    "Apple Mac Pro": {
      socket: "Apple", chipset: "Apple", maxCPUs: 1, maxGPUs: 8, maxDIMMs: 0, maxRAMPerDIMM: 0, supportedRAMTypes: [], tier: "apple", basePower: 0,
      supportedCPUs: ["Apple M2 Ultra"],
      supportedGPUs: ["Apple M2 Ultra (64GB)", "Apple M2 Ultra (128GB)", "Apple M2 Ultra (192GB)"]
    },
    "Apple MacBook Pro": {
      socket: "Apple", chipset: "Apple", maxCPUs: 1, maxGPUs: 1, maxDIMMs: 0, maxRAMPerDIMM: 0, supportedRAMTypes: [], tier: "apple", basePower: 0,
      supportedCPUs: ["Apple M5", "Apple M4 Pro", "Apple M4 Max"],
      supportedGPUs: ["Apple M5 (16GB)", "Apple M5 (24GB)", "Apple M5 (32GB)", "Apple M4 Pro (16GB)", "Apple M4 Pro (24GB)", "Apple M4 Pro (32GB)", "Apple M4 Pro (36GB)", "Apple M4 Pro (48GB)", "Apple M4 Max (36GB)", "Apple M4 Max (48GB)", "Apple M4 Max (64GB)", "Apple M4 Max (128GB)"]
    },
  },
  RAM_TYPES: {
    "DDR4-3200": { bandwidth: 25.6, powerPerDIMM: 3 },
    "DDR4-3600": { bandwidth: 28.8, powerPerDIMM: 3.5 },
    "DDR5-4800": { bandwidth: 38.4, powerPerDIMM: 5 },
    "DDR5-5600": { bandwidth: 44.8, powerPerDIMM: 6 },
    "DDR5-6000": { bandwidth: 48.0, powerPerDIMM: 6.5 },
    "DDR5-6400": { bandwidth: 51.2, powerPerDIMM: 7 },
    "DDR5-7200": { bandwidth: 57.6, powerPerDIMM: 8 },
    "LPDDR5-6400": { bandwidth: 51.2, powerPerDIMM: 0 },
    "LPDDR5X-7500": { bandwidth: 60.0, powerPerDIMM: 0 },
  },
  RAM_AMOUNTS: [16, 32, 64, 128, 256, 512, 1024, 2048],
  MODEL_ATTENTION: {
    "Command A": {
      attention: "MHA",
      embeddings: "rotary",
    },
    "Command R": {
      attention: "MHA",
      embeddings: "rotary",
    },
    "Command R Plus": {
      attention: "MHA",
      embeddings: "rotary",
    },
    "Command R7B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "DeepSeek-R1 1.5B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "DeepSeek-R1 3B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "DeepSeek-R1 7B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "DeepSeek-R1 8B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "DeepSeek-R1 14B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "DeepSeek-R1 32B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "DeepSeek-R1 70B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "DeepSeek-R1 671B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "DeepSeek-R1-0528 671B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "DeepSeek-V3 671B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "DeepSeek-V3.2 685B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "DeepSeek-V3.2-Speciale 685B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Devstral-Small-24B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Devstral-2-123B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Devstral-Small-2-24B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "ERNIE-4.5-0.3B": {
      attention: "MHA",
      embeddings: "learned",
    },
    "ERNIE-4.5-0.3B-Base": {
      attention: "MHA",
      embeddings: "learned",
    },
    "ERNIE-4.5-21B-A3B": {
      attention: "MoE",
      embeddings: "learned",
    },
    "ERNIE-4.5-21B-A3B-Base": {
      attention: "MoE",
      embeddings: "learned",
    },
    "ERNIE-4.5-300B-A47B": {
      attention: "MoE",
      embeddings: "learned",
    },
    "ERNIE-4.5-300B-A47B-Base": {
      attention: "MoE",
      embeddings: "learned",
    },
    "ERNIE-4.5-VL-28B-A3B": {
      attention: "MoE",
      embeddings: "learned",
    },
    "ERNIE-4.5-VL-28B-A3B-Base": {
      attention: "MoE",
      embeddings: "learned",
    },
    "ERNIE-4.5-VL-424B-A47B": {
      attention: "MoE",
      embeddings: "learned",
    },
    "ERNIE-4.5-VL-424B-A47B-Base": {
      attention: "MoE",
      embeddings: "learned",
    },
    "Falcon-1B": {
      attention: "MHA",
      embeddings: "alibi",
    },
    "Falcon-3B": {
      attention: "MHA",
      embeddings: "alibi",
    },
    "Falcon-7B": {
      attention: "MHA",
      embeddings: "alibi",
    },
    "Falcon-40B": {
      attention: "MHA",
      embeddings: "alibi",
    },
    "Falcon-180B": {
      attention: "MHA",
      embeddings: "alibi",
    },
    "Falcon2-11B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Falcon3-1B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Falcon3-3B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Falcon3-7B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Falcon3-10B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Gemma 1 2B": {
      attention: "MQA",
      embeddings: "rope",
    },
    "Gemma 1 7B": {
      attention: "MQA",
      embeddings: "rope",
    },
    "Gemma 2 2B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Gemma 2 9B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Gemma 2 27B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Gemma 3 1B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Gemma 3 4B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Gemma 3 12B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Gemma 3 27B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Gemma 3n E2B IT": {
      attention: "GQA",
      embeddings: "rope",
    },
    "GLM-4.5-355B-A32B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "GLM-4.5-Air-106B-A12B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "GLM-4.6-355B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "GLM-4.7-355B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "GLM-5-744B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "GPT-oss-20B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "GPT-oss-120B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Hunyuan-A13B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "InternLM3-8B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Kimi-Dev-72B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Kimi K2-Base": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Kimi K2-Instruct": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Kimi K2.5": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Kimi-VL-A3B-Instruct": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Kimi-VL-A3B-Thinking": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Llama 3 8B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Llama 3 70B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Llama 3.1 8B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Llama 3.1 70B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Llama 3.1 405B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Llama 3.2 1B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Llama 3.2 3B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Llama 3.3 70B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Llama 4 Scout": {
      attention: "GQA",
      embeddings: "irope",
    },
    "Llama 4 Maverick": {
      attention: "GQA",
      embeddings: "irope",
    },
    "Llama 4 Behemoth": {
      attention: "GQA",
      embeddings: "absolute",
    },
    "MiMo-V2-Flash-309B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "MiniMax-M2-230B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "MiniMax-M2.1-230B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Ministral-3-3B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Ministral-3-8B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Ministral-3-14B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Ministral-8B-2410": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Mistral-7B-Instruct-v0.1": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Mistral-7B-Instruct-v0.2": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Mistral-7B-v0.1": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Mistral-Large-2407": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Mistral-Large-3-675B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Mistral-Small-2501": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Mistral-Small-3.1-24B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Mistral-Small-3.2-24B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Mixtral-8x7B-v0.1": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Mixtral-8x22B-v0.1": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Nemotron-3-Nano-30B-A3B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "OLMo-2-1B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "OLMo-2-7B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "OLMo-2-13B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "OLMo-2-32B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "OLMo-3-7B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "OLMo-3-32B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "Phi-1": {
      attention: "MHA",
      embeddings: "learned",
    },
    "Phi-1.5": {
      attention: "MHA",
      embeddings: "learned",
    },
    "Phi-2": {
      attention: "MHA",
      embeddings: "learned",
    },
    "Phi-3-mini": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Phi-3-small": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Phi-3-medium": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Phi-4": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Phi-4-Mini": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Phi-4 Reasoning Plus": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2-0.5B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2-1.5B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2-7B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2-72B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2.5-0.5B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2.5-1.5B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2.5-3B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2.5-7B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2.5-14B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2.5-32B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen2.5-72B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen3-0.6B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen3-1.7B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen3-4B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen3-8B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen3-14B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen3-30B-A3B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Qwen3-32B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "Qwen3-235B-A22B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Qwen3 235B A22B Thinking": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Qwen3 Coder 480B A35B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "Qwen3.5-397B-A17B": {
      attention: "MoE",
      embeddings: "rope",
    },
    "QwQ-32B": {
      attention: "GQA",
      embeddings: "rope",
    },
    "SmolLM2-135M": {
      attention: "GQA",
      embeddings: "rope",
    },
    "SmolLM2-360M": {
      attention: "GQA",
      embeddings: "rope",
    },
    "SmolLM2-1.7B": {
      attention: "MHA",
      embeddings: "rope",
    },
    "SmolLM3-3B": {
      attention: "GQA",
      embeddings: "rope",
    },
  },
}
